{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abca1fac",
   "metadata": {},
   "source": [
    "### **TOOLS !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d43287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from ollama import Client\n",
    "from groq import Groq\n",
    "import gradio as gr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c951039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138e12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "MODEL_GPT = 'gpt-4.1-nano'\n",
    "\n",
    "client = Client()\n",
    "MODEL_LLAMA = 'llama3.1:8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca5ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI.\"\n",
    "system_message += \"Give shoert, courteous answers, no more than one sentence.\"\n",
    "system_message += \"Always be accurate. If you dont know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2216bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{'role': 'system', 'content': system_message}]\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({'role': 'user', 'content': user_message})\n",
    "        messages.append({'role': 'assistant', 'content': assistant_message})\n",
    "    messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "    response = openai.chat.completions.create(model=MODEL_GPT, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfc41f",
   "metadata": {},
   "source": [
    "### **Tools**  \n",
    "+ Tools are incredibly powerful feature provided by the frontier LLMs.\n",
    "+ With tools, you can write a function, and have the LLM call that function as part of its response.\n",
    "+ Sounds almost spooky.., we are giving it power to run code on our machine ??\n",
    "+ Well, kinda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c340e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {'london': '$799', 'paris': '$899', 'tokyo': '$1400', 'berlin': '$499'}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e6af95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for Berlin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'$499'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is it working..? Absolutely yes!!\n",
    "\n",
    "get_ticket_price('Berlin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ab96e",
   "metadata": {},
   "source": [
    "> There's a particular dictionary structure that's required to describe our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function\n",
    "\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\", \n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'how much does it take to this city?'\", \n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\", \n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\", \n",
    "                \"description\": \"The city that the customer wants to travel to.\", \n",
    "            }, \n",
    "        }, \n",
    "        \"required\": [\"destination_city\"], \n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac1cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this is included in a list of tools\n",
    "\n",
    "tools = [{'type': 'function', 'function': price_function}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506fffe",
   "metadata": {},
   "source": [
    "### **Getting OpenAI to use our Tool**  \n",
    "+ There's some fiddly stuff to allow OpenAI \"to call our tool\".\n",
    "+ What we actually do is give the LLM the opportunity to inform us that it wants us to run the tool. \n",
    "+ Here's how the new chat function looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7923a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{'role': 'system', 'content': system_message}]\n",
    "    for user, assistant in history:\n",
    "        messages.append({'role': 'user', 'content': user})\n",
    "        messages.append({'role': 'assistant', 'content': assistant})\n",
    "    messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "    response = openai.chat.completions.create(model=MODEL_GPT, messages=messages, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response, city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = openai.chat.completions.create(model=MODEL_GPT, messages=messages)\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba5b0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "    response = {\n",
    "        'role': 'tool', \n",
    "        'content': json.dumps({'destination_city': city, 'price': price}), \n",
    "        'tool_call_id': message.tool_calls[0].id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for London\n",
      "Tool get_ticket_price called for Tokyo\n",
      "Tool get_ticket_price called for Berlin\n",
      "Tool get_ticket_price called for Timbuktu\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
