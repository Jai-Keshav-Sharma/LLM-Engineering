{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d60885",
   "metadata": {},
   "source": [
    "### **Gradio Chat Function and Prompting Basics !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb6ba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Engineering\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from ollama import Client\n",
    "from groq import Groq\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f70a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3b697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "MODEL_GPT='gpt-4.1-nano'\n",
    "\n",
    "client = Client()\n",
    "MODEL_LLAMA='llama3.1:8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84bc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e3d2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{'role': 'system', 'content': system_message}]\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({'role': 'user', 'content': user_message})\n",
    "        messages.append({'role': 'assistant', 'content': assistant_message})\n",
    "    messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "    print(\"History is: \",)\n",
    "    print(history)\n",
    "    print(\"And message is: \")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1138f",
   "metadata": {},
   "source": [
    "#### **Now the Gradio Magic !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced62d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is: \n",
      "[]\n",
      "And message is: \n",
      "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hi there'}]\n",
      "History is: \n",
      "[['hi there', 'Hello! How can I assist you today?']]\n",
      "And message is: \n",
      "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hi there'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'my name is keshav'}]\n",
      "History is: \n",
      "[['hi there', 'Hello! How can I assist you today?'], ['my name is keshav', \"Hi Keshav! It's great to meet you. How can I assist you today?\"]]\n",
      "And message is: \n",
      "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hi there'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'my name is keshav'}, {'role': 'assistant', 'content': \"Hi Keshav! It's great to meet you. How can I assist you today?\"}, {'role': 'user', 'content': 'what is my name ?'}]\n",
      "History is: \n",
      "[['hi there', 'Hello! How can I assist you today?'], ['my name is keshav', \"Hi Keshav! It's great to meet you. How can I assist you today?\"], ['what is my name ?', 'Your name is Keshav.']]\n",
      "And message is: \n",
      "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hi there'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'my name is keshav'}, {'role': 'assistant', 'content': \"Hi Keshav! It's great to meet you. How can I assist you today?\"}, {'role': 'user', 'content': 'what is my name ?'}, {'role': 'assistant', 'content': 'Your name is Keshav.'}, {'role': 'user', 'content': 'i want to buy a tie'}]\n",
      "History is: \n",
      "[['hi there', 'Hello! How can I assist you today?'], ['my name is keshav', \"Hi Keshav! It's great to meet you. How can I assist you today?\"], ['what is my name ?', 'Your name is Keshav.'], ['i want to buy a tie', 'That sounds like a great choice! Do you have any specific style, color, or budget in mind for the tie? I can help you find some options or give you tips on choosing the perfect tie.']]\n",
      "And message is: \n",
      "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hi there'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'my name is keshav'}, {'role': 'assistant', 'content': \"Hi Keshav! It's great to meet you. How can I assist you today?\"}, {'role': 'user', 'content': 'what is my name ?'}, {'role': 'assistant', 'content': 'Your name is Keshav.'}, {'role': 'user', 'content': 'i want to buy a tie'}, {'role': 'assistant', 'content': 'That sounds like a great choice! Do you have any specific style, color, or budget in mind for the tie? I can help you find some options or give you tips on choosing the perfect tie.'}, {'role': 'user', 'content': 'i want a red one'}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2afd75b",
   "metadata": {},
   "source": [
    "> At each conversation turn, the `history` as well as the `messages` are being updated, and then the whole `messages` list is passed to the LLM before genereating responses.  \n",
    "> This `messages` list is acting as a Context to the LLM to predict the next most likely tokens.\n",
    "---\n",
    "> It creates a false sense of understanding that the LLM or the Chatbot that we are talking to has memory and is remebering whatever we say."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f9077",
   "metadata": {},
   "source": [
    "#### **A one-shot prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64ae608",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should to genytly encourage the \\\n",
    "    customer to try items that are on sale. Hats are 60% off, and most of the items are 50% off. \\\n",
    "        For example, if the customer says, 'I am looking to buy a hat', \\\n",
    "            you could try something like, 'Wonderful - we have lots of hats - including several that are part of our sales event. \\\n",
    "                Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e776d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{'role': 'system', 'content': system_message}]\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({'role': 'user', 'content': user_message})\n",
    "        messages.append({'role': 'assistant', 'content': assistant_message})\n",
    "    messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "    stream = client.chat(model=MODEL_LLAMA, messages=messages, stream=True)\n",
    "\n",
    "    response=\"\"\n",
    "\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content'] or ''\n",
    "        yield response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41f4160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4f1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not in sale today, \\\n",
    "    but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1eccf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant in a clothes store. You should to genytly encourage the     customer to try items that are on sale. Hats are 60% of, and most of the items are 50% off.         For example, if the customer says, 'I am looking to buy a hat',             you could try something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.                 Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not in sale today,     but remind the customer to look at hats!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcd7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06e69e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant in a clothes store. You should to genytly encourage the     customer to try items that are on sale. Hats are 60% off, and most of the items are 50% off.         For example, if the customer says, 'I am looking to buy a hat',             you could try something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.                 Encourage the customer to buy hats if they are unsure what to get.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c198df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get hacky: appending a system_message in between the messages list\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{'role': 'system', 'content': system_message}]\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({'role': 'user', 'content': user_message})\n",
    "        messages.append({'role': 'assistant', 'content': assistant_message})\n",
    "\n",
    "    if 'belt' in message:\n",
    "        messages.append({'role': 'system', 'content': \"For added context, the store does not sell belts. \\\n",
    "                         But be sure to point out other items on sale.\"})\n",
    "\n",
    "    messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "    stream = client.chat(model=MODEL_LLAMA, messages=messages, stream=True)\n",
    "\n",
    "    response=\"\"\n",
    "\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content'] or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2556ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d1599",
   "metadata": {},
   "source": [
    "> We can add a `system_message` in between the messages list, instead of adding it in the beginning of the list, and in this case, it's working absolutely well. \n",
    "\n",
    "> But it is not suggested at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0ffa1",
   "metadata": {},
   "source": [
    "### How LLMs Understand User/Assistant Message Dictionaries\n",
    "\n",
    "#### The Deep Dive: From Message Structure to Token Prediction\n",
    "\n",
    "When you pass a conversation like this to an LLM:\n",
    "```\n",
    "[\n",
    "    {'role': 'user', 'content': 'What is the capital of France?'},\n",
    "    {'role': 'assistant', 'content': 'Paris is the capital of France.'},\n",
    "    {'role': 'user', 'content': 'Tell me about its history.'}\n",
    "]\n",
    "```\n",
    "\n",
    "**Here's exactly what happens inside the LLM:**\n",
    "\n",
    "#### 1. **Template Formatting**\n",
    "The API converts your message dictionary into a standardized chat template:\n",
    "```\n",
    "<|im_start|>user\n",
    "What is the capital of France?<|im_end|>\n",
    "<|im_start|>assistant  \n",
    "Paris is the capital of France.<|im_end|>\n",
    "<|im_start|>user\n",
    "Tell me about its history.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "#### 2. **Tokenization Into Numbers**\n",
    "Every piece of text becomes a unique number:\n",
    "- `<|im_start|>` → `50258`\n",
    "- `user` → `4312` \n",
    "- `What` → `2061`\n",
    "- `is` → `318`\n",
    "- `the` → `262`\n",
    "- `capital` → `3139`\n",
    "- And so on...\n",
    "\n",
    "#### 3. **Role Understanding Through Pattern Recognition**\n",
    "The LLM learned during training that:\n",
    "- **Numbers following `50258` + `4312`** (user marker) represent human questions/requests\n",
    "- **Numbers following `50258` + `8796`** (assistant marker) represent AI responses\n",
    "- **The sequence matters**: assistant tokens should logically respond to preceding user tokens\n",
    "\n",
    "#### 4. **Attention Mechanism Connects the Dots**\n",
    "When predicting the next token after \"Tell me about its history\":\n",
    "- The model **simultaneously looks at ALL previous tokens**\n",
    "- It notices the pattern: user asked about \"capital of France\" → assistant said \"Paris\" → user wants \"its history\"\n",
    "- Through attention weights, it connects \"its\" back to \"Paris\" mentioned earlier\n",
    "- It understands the conversation flow and context\n",
    "\n",
    "#### 5. **Context Window Processing**\n",
    "- The entire token sequence is processed **as one big input**\n",
    "- Each position gets encoded with both its content AND its position in the sequence\n",
    "- The model learns that tokens appearing after assistant markers should be helpful responses\n",
    "\n",
    "#### 6. **Next Token Prediction**\n",
    "Based on all this context, the model predicts:\n",
    "- \"The\" (high probability - articles often start historical descriptions)\n",
    "- \"Paris\" (medium probability - could repeat the subject)\n",
    "- \"French\" (medium probability - relevant to France)\n",
    "\n",
    "**The Key Insight:** LLMs don't truly \"understand\" roles like humans do. They recognize statistical patterns in number sequences that correspond to different conversation participants, and generate responses that fit the learned patterns of helpful assistant behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
