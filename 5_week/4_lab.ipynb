{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1cf89b",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "+ A question answering agent that is an expert knowledge worker.\n",
    "+ To be used by Insurellm, an Insurance Tech Company.\n",
    "+ The agent needs to be accurate and the solution should be low cost.\n",
    "---\n",
    "This project will use RAG to ensure our question/answering assistant's high accuracy.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969c0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bd86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6835bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a0399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Loading the documents using LangChain loaders\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "documents = []\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata['doc_type'] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f528e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Chunking\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be85afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB Created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "# Step 3. Embedding and putting the chunks in Vector Database\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# is db already exists, delete it \n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create Vector Store\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vector DB Created with {vectorstore._collection.count()} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5e65e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# collecting one documents and finding how many dimensions it has !!\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ced0a5",
   "metadata": {},
   "source": [
    "### **Time to bring it all together using LangChain !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11336909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KESHAV\\AppData\\Local\\Temp\\ipykernel_9152\\1927888662.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# The final step \n",
    "\n",
    "# Create a new chat model with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# The retriever is an abstraction over the vector store that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it all together: set up the conversation chain with the gpt-4.1-nano llm, vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out !!\n",
    "\n",
    "query = \"Can you describe Insurellm in few sentences ?\"\n",
    "result = conversation_chain.invoke({'question': query})\n",
    "\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a17140",
   "metadata": {},
   "source": [
    "### Gradio UI !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcd38a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query, history):\n",
    "    result = conversation_chain.invoke({'question': query})\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a17a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695af20",
   "metadata": {},
   "source": [
    "#### As the LangChain `ConversationBufferMemory()` and all similar packages have been deprecated, here's an alternative !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6745ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver \n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c43a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KESHAV\\AppData\\Local\\Temp\\ipykernel_23972\\3651799587.py:7: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  retriever_tool = retriever.as_tool(\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LLM\n",
    "llm = ChatOpenAI(temperature=0.7, model=MODEL)\n",
    "\n",
    "# Setting up the memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Reteriever object\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# to be passed as a tool to the agent\n",
    "retriever_tool = retriever.as_tool(\n",
    "    name=\"retriever\", \n",
    "    description=\"Retrieve relevant documents from the knowledge base.\"\n",
    ")\n",
    "\n",
    "# Agent creation\n",
    "agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=[retriever_tool], \n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# configurables\n",
    "config = {'configurable': {'thread_id': '1'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4433742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat Function for gradio UI\n",
    "\n",
    "def chat_with_agent(message, history):\n",
    "    \"\"\"\n",
    "    Simple chat function for the React agent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the user message\n",
    "    user_message = {'role': 'user', 'content': message}\n",
    "        \n",
    "    # Get response from agent\n",
    "    result = agent.invoke({'messages': [user_message]}, config)\n",
    "        \n",
    "    # Extract the assistant's response\n",
    "    response = result['messages'][-1].content\n",
    "        \n",
    "    return response\n",
    "\n",
    "\n",
    "gr.ChatInterface(fn=chat_with_agent, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
